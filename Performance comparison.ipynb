{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Assignment_2-Q2.2_Q3.ipynb","version":"0.3.2","provenance":[{"file_id":"1PleheYAPecfrHvvWWQRr3nuIVFwf9NPq","timestamp":1560011748339},{"file_id":"1yPAmLivmOad7543fKBABQOdJZOeYOzuN","timestamp":1558381378844}],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CCcmbz0UU7mR"},"source":["***\n","## Question 3: Performance comparison (3pt)\n","\n","\n","**a)** What accuracy would random guessing achieve (on average) on this dataset? Motivate your answer briefly."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"BKGDydqsVVX1"},"source":["Random guessing would mean we pick the right class by choosing a random selection out of N options, where each option has an equal chance of being chosen. Since we are doing N-way one-shot learning, this probability would be 1/N. We took N = 20, so random guessing would on average give an accuracy of 1/20 = 0.05 (or 5%)."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5KLXRv-eV04Q"},"source":["**b)** Discuss and compare the performances of networks in tasks 1.1, 1.2 and 2.2. Briefly motivate and explain which task would be expected the highest accuracy. Explain the reasons of the accuracy difference if there are any. If there is almost no difference accuracy, explain the reason for that."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"71kTHFBkcjp8"},"source":["\"Evaluate the performance of the network on 20-way one-shot learning tasks. Do this by generating 250 random tasks and obtain the average accuracy for each evaluation round. Use the remaining 20 classes that were not used for training. The model should perform better than random guessing.\"\n","\n","\n","__1.1: Siamese network (with one-shot learning)__\n","- On a 20-way one-shot learning evaluation, the siamese CNN network obtained a best average accuracy of 22.0%. In this network, the distances between images are calculated with L1 distances(least absolute errors), also called Manhattan distances. From our literature research, we found that the two distance measures(Manhattan and Euclidian) are comparable, but Euclidian distances are more suitable when the arrays have the same shape, while the reverse seems to be true when the data has high dimensionality(ref: https://bib.dbvis.de/uploadedFiles/155.pdf). Since our data has relatively low dimensionality, we reason that L2 distances may be more appropriate.\n","\n","__1.2: Neural codes (with one-shot learning)__\n","- The neural codes from the CNN obtained an accuracy of 0.402125 (40%). Due to reasons cited above, we think this is to be expected\n","\n","__2.2: Triplet neural codes (with one-shot learning)__\n","- Our neural codes obtained from the triplet model with L2 distances obtained the highest accuracy of 58%. \n","\n","We believe this order of the 3 accuracies was expected, given the following reasoning. \n","\n","Siamese and twin network take into account distances of each test img from the remaining imgs of same category, as well as other categories. The siamese network is trained to either minimise or maximise the distance between 2 images, depending on whether or not they belong to the same class. The triplet network on the other hand learns to identify which of the 2 images is the positive. It tries to maximise and minimise the appropriate distances at the same time. In simpler words, it tries to learn HOW to learn to differentiate between similar and different. Thus, it is bound to be better at one-shot learning tests done on a test set which the model has not seen before."]}]}